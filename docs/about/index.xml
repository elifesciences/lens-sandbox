<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article SYSTEM "journalpub-oasis3.dtd">

<article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:oasis="http://docs.oasis-open.org/ns/oasis-exchange/table" article-type="research-article" dtd-version="3.0" xml:lang="EN">
  <front>
    <journal-meta>
      <journal-id journal-id-type="nlm-ta">lens-dev</journal-id>

      <journal-title-group>
        <journal-title>Lens</journal-title>
      </journal-title-group>
      <issn pub-type="epub">2050-084X</issn>
      <publisher>
        <publisher-name>eLife Lens</publisher-name>
      </publisher>
    </journal-meta>
    <article-meta>

      <title-group>
        <article-title>eLife Lens: A novel way of seeing content</article-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author">
          <name>
            <surname>Grubisic</surname>
            <given-names>Ivan</given-names>
          </name>
          <xref ref-type="aff" rid="aff1">1</xref>
          <xref ref-type="fn" rid="con1"/>
        </contrib>

        <contrib contrib-type="author">
          <name>
            <surname>Aufreiter</surname>
            <given-names>Michael</given-names>
          </name>
          <xref ref-type="aff" rid="aff2">2</xref>
          <xref ref-type="fn" rid="con2"/>
        </contrib>

        <contrib contrib-type="author">
          <name>
            <surname>Buchtala</surname>
            <given-names>Oliver</given-names>
          </name>
          <xref ref-type="aff" rid="aff2">2</xref>
          <xref ref-type="fn" rid="con3"/>
        </contrib>

        <contrib contrib-type="author">
          <name>
            <surname>Nott</surname>
            <given-names>Graham</given-names>
          </name>
          <xref ref-type="aff" rid="aff3">3</xref>
          <xref ref-type="fn" rid="con4"/>
        </contrib>

        <contrib contrib-type="author">
          <name>
            <surname>Close</surname>
            <given-names>Rebecca</given-names>
          </name>
          <xref ref-type="aff" rid="aff6">6</xref>
          <xref ref-type="fn" rid="con8"/>
        </contrib>

        <contrib contrib-type="author">
          <name>
            <surname>Korosec</surname>
            <given-names>Samo</given-names>
          </name>
          <xref ref-type="aff" rid="aff4">4</xref>
          <xref ref-type="fn" rid="con6"/>
        </contrib>

        <contrib contrib-type="author">
          <name>
            <surname>Hamilton</surname>
            <given-names>Ian</given-names>
          </name>
          <xref ref-type="aff" rid="aff5">5</xref>
          <xref ref-type="fn" rid="con7"/>
        </contrib>

        <contrib contrib-type="author">
          <name>
            <surname>Mulvany</surname>
            <given-names>Ian</given-names>
          </name>
          <xref ref-type="aff" rid="aff3">3</xref>
          <xref ref-type="fn" rid="con5"/>
        </contrib>

        <aff id="aff1">
          <label>1</label>
          <institution>Li Ka Shing Center For Biomedical and Health Sciences, CIRM Center of Excellence, University of California, Berkeley</institution>,
          <addr-line>
          <named-content content-type="city">Berkeley</named-content>
          </addr-line>,
          <country>United States</country>
        </aff>

        <aff id="aff2">
          <label>2</label>
          <institution>Substance</institution>,
          <addr-line>
          <named-content content-type="city">Linz</named-content>
          </addr-line>,
          <country>Austria</country>
        </aff>

        <aff id="aff3">
          <label>3</label>
          <institution>eLife Publications</institution>,
          <addr-line>
          <named-content content-type="city">Cambridge</named-content>
          </addr-line>,
          <country>United Kingdom</country>
        </aff>

        <aff id="aff4">
          <label>4</label>
          <institution>froodee design bureau</institution>,
          <addr-line>
          <named-content content-type="city">Vienna</named-content>
          </addr-line>,
          <country>Austria</country>
        </aff>

        <aff id="aff5">
          <label>5</label>
          <institution>ripe</institution>,
          <addr-line>
          <named-content content-type="city">Washington DC</named-content>
          </addr-line>,
          <country>United States</country>
        </aff>

        <aff id="aff6">
          <label>6</label>
          <institution>Landes Bioscience</institution>,
          <addr-line>
          <named-content content-type="city">Houston, Texas</named-content>
          </addr-line>,
          <country>United States</country>
        </aff>

        <fn fn-type="con" id="con1">
          <p>Conception of the idea, XML to JSON conversion, drafting and revising the article</p>
        </fn>

        <fn fn-type="con" id="con2">
          <p>Design and implementation of Lens, revising the article</p>
        </fn>

        <fn fn-type="con" id="con3">
          <p>Reference implementation of the new browser-based NLM converter</p>
        </fn>

        <fn fn-type="con" id="con4">
          <p>Implementation of the deployment workflow, revising the article</p>
        </fn>

        <fn fn-type="con" id="con5">
          <p>Project coordination</p>
        </fn>

        <fn fn-type="con" id="con6">
          <p>Advice on UX Design, mockups to support UI development</p>
        </fn>

        <fn fn-type="con" id="con7">
          <p>Advice on the UI Design, revising the article</p>
        </fn>

        <fn fn-type="con" id="con8">
          <p>Integration of Lens at Landes Bioscience. Advice on the design of the NLM converter</p>
        </fn>

      </contrib-group>
      <abstract>
        <p>eLife Lens provides a novel way of looking at content on the web (see <ext-link xlink:href="http://www.elifesciences.org/lens/">introduction post</ext-link>). It is designed to make life easier for researchers, reviewers, authors and readers. For example, have you tried to look at a figure in an online article, while at the same time trying to see what the author says about the figure, jumping all around the article, losing track of what you were looking for in the first place? The reason for this is that most online research articles are published in a fixed digital version of the original paper. With eLife Lens, we take full advantage of the dynamic nature of HTML combined with javascript (<xref ref-type="fig" rid="video1">Video 1</xref>).</p>
      </abstract>

    </article-meta>
  </front>
  <body>
    <sec id="s1" sec-type="motivation">
      <title>Motivation</title>
      <p>Working with digital documents has been difficult for the most part, because they come in presentation-centric formats, optimized for print and with the intent to have the same display across multiple devices. Ultimately, the display of these documents is preserved to allow the user to print the exact same document across any device. Content today, however, is no longer being printed out readily; instead, it is read on a variety of platforms spanning computers and mobile devices. The limitations presented by different screen sizes, the lack of tactile feedback that comes from flipping between pages and inability to purely focus on the author’s arguments are problems present in all disciplines.</p>
      <p>The web browser provides a unified platform for viewing content. Instead of binding the content to a presentation focused format, we can view the content as data. Thereby making the content readily accessible by utilizing a defined data structure. This data structure can then be processed in similar ways to databases -- allowing users to query the content, create new content and build new tools.</p>
      <p>We have decided to focus our initial efforts on applying the data-centric representation of content to the scientific literature. The Open Access scientific community has standardized much of their content into a formally annotated XML format, making it easier to gain access to a large library of articles. The content of scientific articles is a combination of text, figures, tables, videos and references which are used to form the author’s argument. Scientific arguments are also rarely linear in nature, making it difficult to follow an argument when all of the content is not readily viewable on a digital device. The large amount of available data and non-linear format of the articles, provided us with the ideal start to test the flexibility of our data-centric approach. With the release of Lens, we would like to not only promote the data-centric representation of scientific content, but that of any content, which would be optimized for viewing on web clients.</p>
    </sec>

    <sec id="s2" sec-type="presentation">
      <title>The Presentation of Lens</title>
      <p>When designing Lens, we worked to provide a flexible experience that supports a variety of use cases. On the far left the document map (<xref ref-type="fig" rid="fig2">Figure 2A</xref>) identifies the readers position in the context of the whole article. It also outlines each paragraph in the article. The left panel includes all of the textual content of the article (<xref ref-type="fig" rid="fig2">Figure 2B</xref>). The right panel includes the resources (<xref ref-type="fig" rid="fig2">Figure 2C</xref>). The resources are defined as the table of contents, figures, tables, videos, supplemental data, references and information about the article. By splitting apart the content into individual panels, we enable the reader to focus on multiple content bits at the same time.</p>
      <p>When reading scientific articles, it is rare for a reader to read straight through the text and figures in order. Instead, they may look to get a quick overview of the paper by looking at the figures first. By splitting the figures out from the text, the reader can transition between the two independently of one another. This allows the reader to simultaneously view exactly the content they want to focus on at that moment.</p>
<!--      <p>Two viewing modes that are implicitly supported are text-centric and resource-centric viewing. Text-centric viewing creates a microscale reading experience by limiting the viewable content to a singular content node. When a figure or publication label is selected within a text node, only the figures or publications that are referenced within that paragraph will be displayed in their appropriate resource sections (<xref rid="fig3" ref-type="fig">Figure 3</xref>, <xref rid="fig4" ref-type="fig">Figure 4</xref>). Selection of any figure or publication will color the paragraphs in the document map (<xref rid="fig2" ref-type="fig">Figure 2A</xref>) that include a reference to the selected resource.</p>
-->      <p>The presentation of the article information has also been redesigned to organize the information in a uniform way. Each publisher will have information like the author’s impact statement, article keywords, major dataset links, etc. placed in different positions of their HTML or PDF versions of the articles. The aim is to distill all of the article information into organized subsections that can be displayed together on individual cards. The information cards create a uniform viewing experience making it easy to quickly find what the reader is looking for.</p>
      <p>In addition to simplifying the reading experience of research articles, Lens also provides useful tools to share content with others. Each of the content nodes in the data structure are deep-linked which means that each state is defined by a given URL. Sharing a specific URL will take the new reader to the exact same state in their browser which will facilitate the ease discussions about the content.</p>
    </sec>

    <sec id="s3" sec-type="format">
    	<title>The Lens Article</title>
      <p>We are convinced that there is a need to break with traditional presentation-centric formats, by considering content as data and making it accessible in new ways. Therefore with the release of Lens, we’d like to promote a data-centric representation of scientific content, optimized for being consumed by web-clients, the <ext-link xlink:href="http://lens.substance.io/#lens/lens_article">Lens Article</ext-link></p>
      <p>Lens can display any document that conforms to a simple JSON representation. JSON representations are flexible and can be adapted to any type of content. The initial JSON object provides the framework for organizing all of the content in an article. The id and properties keys provide global access if many JSON articles are to be queried for an initial round of filtering. The nodes key contains all of the content and the view key defines indices about what order the nodes will be displayed. The nodes are then linked together via annotations. The annotations are defined by either an explicit reference to a target content node that is contained in its source content code or font styling. <xref ref-type="fig" rid="fig1">Figure 1</xref> outlines a simple example of how to use the JSON representation to compose an article for Lens.</p>

      <p>The creation of an article's JSON depends on a browser-based converter that runs through the XML tags turning them into a smart data structure that powers our viewer. The XML tags define various types of content nodes. Each node contains type, id and content keys that the viewer then uses to define the rendering. All of the keys have their own definitions with regards to how Lens renders them. The translation from the XML to a structured JSON creates a consistent presentation scheme for the content.</p>
    </sec>

    <sec id="s4" sec-type="open_source">
    <title>Open Development</title>
    <p>Lens is open source software. Its codebase has a modular design and is available on <ext-link xlink:href="https://github.com/elifesciences/lens">Github</ext-link>. Lens heavily relies on <ext-link xlink:href="http://substance.io">Substance</ext-link>, an open platform for browser-based document manipulation. We'd like to invite anyone to use Lens for displaying their own content and get involved in the development process. You can report bugs and discuss features on Github or post to our <ext-link xlink:href="https://groups.google.com/forum/#!forum/elife-lens">mailing list</ext-link>. If you are a developer take a glance at the <ext-link xlink:href="http://lens.substance.io/#lens/manual">Lens Manual</ext-link>.</p>
    </sec>

    <sec id="s7" sec-type="credits">
    <title>Credits</title>
    <p>eLife Lens was developed in collaboration between <ext-link xlink:href="http://bioegrad.berkeley.edu/">UC Berkeley</ext-link> graduate student <ext-link xlink:href="http://www.linkedin.com/pub/ivan-grubisic/26/353/739">Ivan Grubisic</ext-link> and eLife. He was supported by Michael Aufreiter from Substance, who helped with the design and implementation of the tool. Samo Korošec (<ext-link xlink:href="http://dribbble.com/froodee">froodee</ext-link>) and Ian Hamilton (ripe) gave advice on the UX/UI design. <ext-link xlink:href="http://www.grahamnott.com/">Graham Nott</ext-link> implemented the deployment workflow and eLife provided support for the project and the initial corpus of documents against which the tool was developed. <ext-link xlink:href="http://www.linkedin.com/pub/rebecca-close/4a/585/224">Rebecca Close</ext-link>, integrated Lens at <ext-link xlink:href="https://www.landesbioscience.com">Landes Bioscience</ext-link> and helped with the redesign of the converter, that now supports all NLM-compatible content.</p>
    </sec>
  </body>
  <back>
    <media id="video1" content-type="glencoe play-in-place height-250 width-310" xlink:href="http://cdn.elifesciences.org/video/eLifeLensIntro2.mp4" mimetype="video" mime-subtype="avi">
      <object-id pub-id-type="doi">10.7554/eLife.00116.004</object-id>
      <label>Video 1.</label>
      <caption>
        <title>Introducing eLife Lens.</title>
        <p>Watch Ian Mulvany from eLife demonstrating Lens.</p>
      </caption>
    </media>
    <fig id="fig1">
      <label>Figure 1.</label>
      <caption>
        <title>The Lens Document Format</title>
        <p>An example of how to begin creating content for Lens. The backbone organizes all of the data in a readily accessible manner. The text node defines the paragraph that may reference figures or publications. The figure node defines the image that will be displayed while the annotation links the figure to the associated text node that references the figure.</p>
      </caption>
      <graphic xlink:href="figure_1.png"/>
    </fig>

    <fig id="fig2">
      <label>Figure 2.</label>
      <caption>
        <title>The workspace of Lens.</title>
        <p><bold>A.</bold> The document map. Each gray bar identifies an individual paragraph within the article. <bold>B.</bold> The article’s text content. <bold>C.</bold> All of the associated resources. This includes the Table of Contents, figures, references and additional information, including meta-data, about the article.</p>
      </caption>
      <graphic xlink:href="figure_2.jpg"/>
    </fig>

    <fig id="fig3">
      <label>Figure 3.</label>
      <caption>
        <title>Focus on a Figure</title>
        <p>Selecting a figure label within the text. The green highlighted camera is a quick link to view all of the figures that are referenced within the selected paragraph. The black arrow to the left of the document map is a visual cue for the reader to quickly return to the paragraph they were on if they navigate away from this to another paragraph that references Figure 5. The reader can select the figure or reference label in the resources panel (Figure 2C) to focus on the paragraphs where a specific figure is mentioned. The document map (Figure 2A) will highlight the paragraphs that reference the selected resource.</p>
      </caption>
      <graphic xlink:href="figure_3.png"/>
    </fig>

    <fig id="fig4">
      <label>Figure 4.</label>
      <caption>
        <title>Focus on a Reference</title>
        <p>Insert Figure 4 caption ...</p>
      </caption>
      <graphic xlink:href="figure_4.png"/>
    </fig>

  </back>
</article>
